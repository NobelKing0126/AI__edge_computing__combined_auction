# 核心代码总结

## 一、竞争比计算 (Competitive Ratio)

### 1.1 竞争比定义

```
竞争比 ρ = SW* / SW_online

其中:
- SW* = 离线最优社会福利（Oracle知道所有任务）
- SW_online = 在线算法社会福利
```

### 1.2 核心类: `CompetitiveRatioCalculator`

**文件位置**: `algorithms/offline_optimal.py`

```python
@dataclass
class BidInfo:
    """投标信息"""
    user_id: int
    layer: int          # 切分层
    uav_id: int         # 目标UAV
    utility: float      # 效用值 η_final
    f_edge: float       # 边缘算力需求
    f_cloud: float      # 云端算力需求
    energy: float       # 能量消耗
    priority_class: str # 优先级类别: high/medium/low


class OfflineOptimalSolver:
    """离线最优问题求解器"""

    def solve(self, bids: List[BidInfo], use_lp_relaxation: bool = True):
        """
        求解离线最优问题

        问题形式:
            max  Σ η_{i,l,j} * x_{i,l,j}
            s.t. 用户唯一性、UAV算力、能量、云端算力约束
        """
        if use_lp_relaxation:
            return self._solve_lp_relaxation(bids)
        else:
            return self._solve_ilp(bids)

    def _solve_lp_relaxation(self, bids: List[BidInfo]):
        """
        LP松弛求解: x ∈ {0,1} → x ∈ [0,1]
        """
        # 目标函数: max Σ η * x  => min -Σ η * x
        c = np.array([-b.utility for b in bids])

        # 构建约束矩阵
        # - 用户唯一性约束
        # - UAV算力约束
        # - UAV能量约束
        # - 云端算力约束

        # 求解LP
        result = linprog(c, A_ub=A_ub, b_ub=b_ub, A_eq=A_eq, b_eq=b_eq,
                         bounds=[(0, 1) for _ in range(n_vars)])

        sw_optimal = -result.fun
        return sw_optimal, allocation


class CompetitiveRatioCalculator:
    """竞争比计算器"""

    def compute(self, bids: List[BidInfo], sw_online: float):
        """
        计算竞争比

        Returns:
            {
                'sw_optimal': 离线最优社会福利,
                'sw_online': 在线算法社会福利,
                'actual_ratio': 实际竞争比,
                'dual_gap': 对偶间隙,
                'gap_percentage': 间隙百分比
            }
        """
        # 求解离线最优
        sw_optimal, _ = self.offline_solver.solve(bids, use_lp_relaxation=True)

        # 实际竞争比
        actual_ratio = sw_optimal / sw_online

        # 对偶间隙
        dual_gap = sw_optimal - sw_online

        # 间隙百分比
        gap_percentage = (sw_optimal - sw_online) / sw_optimal * 100

        return {
            'sw_optimal': sw_optimal,
            'sw_online': sw_online,
            'actual_ratio': actual_ratio,
            'dual_gap': dual_gap,
            'gap_percentage': gap_percentage
        }
```

### 1.3 实验结果

| 用户数 | 竞争比 | Gap% |
|--------|--------|------|
| 20 | 1.411 | 28.2% |
| 30 | 1.395 | 27.8% |
| 40 | 1.469 | 31.7% |
| 50 | 1.432 | 29.9% |

---

## 二、Active Inference 主动推理

### 2.1 状态空间定义 (`state_space.py`)

```python
class ActionType(Enum):
    """行动类型"""
    CONTINUE = "continue"       # 继续执行
    CHECKPOINT = "checkpoint"   # 保存检查点
    REDUCE_POWER = "reduce"     # 降低功率
    HANDOVER = "handover"       # 请求转移
    ABORT = "abort"            # 中止返航


@dataclass
class StateVector:
    """状态向量 s_t = [E_t, T_t, h_t, p_t, d_t, σ_t]"""
    E: float      # 剩余能量 (J)
    T: float      # 已用时间 (s)
    h: float      # 健康度 [0,1]
    p: float      # 任务进度 [0,1]
    d: float      # 距离目标 (m)
    sigma: float  # 不确定性 [0,1]


@dataclass
class ObservationVector:
    """观测向量 o_t = [Ê_t, ĥ_t, q_t, w_t]"""
    E_hat: float  # 能量观测 (J)
    h_hat: float  # 健康度观测 [0,1]
    q: float      # 信道质量 [0,1]
    w: float      # 环境因素 [0,1]
```

### 2.2 生成模型 (`generative_model.py`)

```python
class TransitionModel:
    """状态转移模型"""

    def predict_next_state(self, state: StateVector, action: ActionType, delta_t: float):
        """
        预测下一状态

        状态转移公式:
            E_{t+1} = E_t - P(a_t) × Δt - ε_E
            T_{t+1} = T_t + Δt × κ(a_t)
            h_{t+1} = h_t × λ_h - δ_h(a_t) + ε_h
            p_{t+1} = min(1, p_t + Δt × η(a_t) / T_task)
        """
        P_action = self.action_effects.get_power(action)
        kappa = self.action_effects.get_time_coefficient(action)
        eta = self.action_effects.get_efficiency_factor(action)

        # 能量转移
        E_next = state.E - P_action * delta_t

        # 时间转移
        if action == ActionType.ABORT:
            T_next = state.T
        else:
            T_next = state.T + delta_t * kappa

        # 健康度转移
        h_next = state.h * self.action_effects.lambda_h

        # 进度转移
        progress_delta = delta_t * eta / self.T_task if action != ActionType.ABORT else 0.0
        p_next = min(1.0, state.p + progress_delta)

        return TransitionResult(
            next_state=StateVector(E=E_next, T=T_next, h=h_next, p=p_next, ...),
            energy_consumed=P_action * delta_t,
            time_elapsed=delta_t * kappa,
            progress_made=progress_delta
        )


class LikelihoodModel:
    """似然模型"""

    def compute_likelihood(self, obs: ObservationVector, state: StateVector):
        """
        计算似然 P(o_t | s_t) = N(o_t; g(s_t), Σ_o)
        """
        # 高斯似然
        log_p = -0.5 * (np.log(2 * np.pi * sigma**2) +
                         (obs - mu)**2 / (sigma**2 + EPSILON))
        return log_p
```

### 2.3 信念更新器 (`belief_updater.py`)

```python
class KalmanUpdater:
    """卡尔曼滤波更新器"""

    def update(self, belief: BeliefState, obs: ObservationVector):
        """
        卡尔曼滤波更新: μ_t ← μ_t + K_t (o_t - ô_t)
        """
        # 预测观测
        pred_mean = self.H @ belief.mean.to_numpy()

        # 新息
        innovation = obs_vector - pred_mean

        # 卡尔曼增益
        K = belief.covariance @ self.H.T @ np.linalg.inv(S + EPSILON)

        # 更新均值
        new_mean_vec = belief.mean.to_numpy() + K @ innovation

        # 更新协方差
        new_covariance = (np.eye(6) - K @ self.H) @ belief.covariance @ (np.eye(6) - K @ self.H).T

        return BeliefState(mean=new_state, covariance=new_covariance)


class VariationalUpdater:
    """变分推断更新器"""

    def update(self, belief: BeliefState, obs: ObservationVector):
        """
        变分推断更新: μ ← μ - η × ∇_μ F

        自由能 F = E_Q[-log P(o|s)] + D_KL[Q(s) || P(s)]
        """
        for _ in range(n_iterations):
            # 计算梯度 (数值梯度)
            grad = self._compute_gradient(belief, obs)

            # 梯度下降
            mean_vec = belief.mean.to_numpy() - self.learning_rate * grad

            # 应用约束
            mean_vec = self._apply_constraints(mean_vec)

            new_belief = BeliefState(mean=new_state, covariance=belief.covariance)

        return new_belief
```

### 2.4 轨迹预测器 (`trajectory_predictor.py`)

```python
class TrajectoryPredictor:
    """轨迹预测器"""

    def predict(self, initial_state: StateVector, policy: List[ActionType], method: str):
        """
        预测轨迹

        确定性预测: ŝ_{τ+1} = f(ŝ_τ, a_τ)
        不确定性传播: Σ_{τ+1} = F_τ Σ_τ F_τ^T + Q
        """
        if method == 'deterministic':
            trajectory = self.deterministic.predict(initial_state, policy)
        elif method == 'monte_carlo':
            trajectories = self.mc_sampler.sample_trajectories(initial_state, policy)
            stats = self.mc_sampler.estimate_statistics(trajectories)

        return trajectory, stats
```

### 2.5 自由能计算器 (`free_energy.py`)

```python
class FourComponentCalculator:
    """四分量自由能计算器"""

    def compute_instant_free_energy(self, state: StateVector):
        """
        计算即时自由能

        F_t = w_E × F_t^energy + w_T × F_t^time + w_h × F_t^health + w_p × F_t^progress

        其中:
            F_t^energy = -log(E_t / E_required)
            F_t^time = -log((T_max - T_t) / T_remaining_required)
            F_t^health = -log(h_t × q_t)
            F_t^progress = -log((p_t + ε) / (p_expected + ε))
        """
        F_energy = -np.log(max(state.E, EPSILON) / E_required)
        F_time = -np.log(max(T_max - state.T, EPSILON) / T_remaining)
        F_health = -np.log(max(state.h * channel_quality, EPSILON))
        F_progress = -np.log(max(state.p + EPSILON) / (p_expected + EPSILON))

        F_total = (self.w_E * F_energy + self.w_T * F_time +
                   self.w_h * F_health + self.w_p * F_progress)

        return InstantFreeEnergy(F_total=F_total, F_energy=F_energy, ...)


class ExpectedFreeEnergyCalculator:
    """期望自由能计算器"""

    def compute_expected_free_energy(self, trajectory):
        """
        计算期望自由能

        G(π) = Σ γ^τ × E[F_τ] + α × Σ γ^τ × H[Q(o_τ | π)]
              = G_pragmatic(π) + G_epistemic(π)
        """
        # 实用价值: G_pragmatic = Σ γ^τ × F(ŝ_τ^π)
        G_pragmatic = self._compute_pragmatic_value(trajectory)

        # 认知价值: G_epistemic = Σ γ^τ × H[Q(o_τ | π)]
        G_epistemic = self._compute_epistemic_value(trajectory)

        G_total = G_pragmatic + self.alpha * G_epistemic

        return ExpectedFreeEnergy(G_total=G_total, ...)
```

### 2.6 行动选择器 (`action_selector.py`)

```python
class GreedySelector:
    """贪婪行动选择器"""

    def select(self, evaluations: List[ActionEvaluation]):
        """
        贪婪选择: a* = argmin_a G(a)
        """
        return min(evaluations, key=lambda e: e.G_value)


class SoftmaxSelector:
    """Softmax概率选择器"""

    def select(self, evaluations: List[ActionEvaluation]):
        """
        Softmax选择: P(a) = exp(-β × G(a)) / Σ exp(-β × G(a'))

        参数 β (逆温度):
            - β → 0: 随机选择 (探索)
            - β → ∞: 贪婪选择 (利用)
        """
        G_values = [e.G_value for e in evaluations]
        exp_neg_beta_G = np.exp(-self.beta * np.array(G_values))
        probs = exp_neg_beta_G / np.sum(exp_neg_beta_G)

        idx = np.random.choice(len(evaluations), p=probs)
        return evaluations[idx]
```

### 2.7 感知-行动循环 (`perception_action_loop.py`)

```python
class PerceptionActionLoop:
    """感知-行动循环"""

    def step(self):
        """
        执行一步循环

        流程:
            1. OBSERVE: o_t ← get_observation()
            2. PERCEIVE: Q(s_t) ← update_belief(Q(s_{t-1}), o_t)
            3. PREDICT: For each a: G(a) ← compute_expected_free_energy(predict(Q(s_t), a))
            4. ACT: a* ← select_action({G(a)}), execute(a*)
            5. LEARN: update_model_parameters(o_t, s_t, a*)
            6. LOOP: t ← t + 1
        """
        # Phase 1: OBSERVE
        obs = self.obs_interface.get_observation()

        # Phase 2: PERCEIVE
        self.loop_state.belief = self.belief_updater.update(obs)
        instant_fe = self.fe_calculator.compute_instant(self.loop_state.belief.mean)

        # Phase 3: PREDICT
        for action in list(ActionType):
            eval_result = self.action_selector.evaluate_single_action(
                self.loop_state.belief.mean, action
            )
            self.loop_state.action_evaluations.append(eval_result)

        # Phase 4: ACT
        selected = self.action_selector.select_action(self.loop_state.belief.mean)
        exec_result = self.exec_interface.execute(selected.action)

        # Phase 5: LEARN
        self.history.append({
            'observation': obs,
            'action': selected.action,
            'free_energy': self.loop_state.free_energy,
            'reward': exec_result.reward
        })

        return exec_result
```

### 2.8 Active Inference 核心公式

```
完整生成模型:
    P(õ, s̃, π) = P(π) × Π P(o_τ | s_τ) × P(s_τ | s_{τ-1}, a_{τ-1})

即时自由能 (四分量):
    F_t = w_E × F_t^energy + w_T × F_t^time + w_h × F_t^health + w_p × F_t^progress

期望自由能:
    G(π) = Σ γ^τ × E[F_τ] + α × Σ γ^τ × H[Q(o_τ | π)]
          = G_pragmatic(π) + G_epistemic(π)

贝叶斯信念更新:
    Q(s_t | o_t) ∝ P(o_t | s_t) × Q(s_t)
```

---

## 三、闭式解求解 (Closed-form Solution)

### 3.1 问题定义

给定切分点，优化边缘/云端算力分配，最小化时延

**目标**:
```
min T = T_upload + C_edge/f_edge + T_trans + C_cloud/f_cloud + T_return
```

**约束**:
```
C1 (时延约束): C_edge/f_edge + C_cloud/f_cloud ≤ T_budget
C2 (能量约束): κ_edge·f_edge²·C_edge + κ_cloud·f_cloud²·C_cloud ≤ E_budget
C3 (算力上界): f_edge ≤ f_edge_avail, f_cloud ≤ f_cloud_avail
C4 (非负性): f_edge ≥ 0, f_cloud ≥ 0
```

### 3.2 核心类: `ResourceOptimizer`

**文件位置**: `algorithms/phase2/resource_optimizer.py`

```python
@dataclass
class AllocationResult:
    """资源分配结果"""
    f_edge: float      # 边缘分配算力 (FLOPS)
    f_cloud: float     # 云端分配算力 (FLOPS)
    T_total: float     # 总时延 (s)
    E_total: float     # 总能耗 (J)
    feasible: bool     # 是否可行
    case_used: int     # 使用的闭式解类型 (1-4)


class ResourceOptimizer:
    """凸优化资源分配器"""

    def _solve_case1(self, f_edge_avail, f_cloud_avail):
        """
        Case 1: 两约束均不激活
        取算力上界
        """
        return f_edge_avail, f_cloud_avail

    def _solve_case2(self, C_edge, C_cloud, T_budget, f_edge_avail, f_cloud_avail):
        """
        Case 2: 仅时延约束激活
        按计算量比例分配时间预算

        最优解: f_edge* = C_edge / (T_budget · r)
                 f_cloud* = C_cloud / (T_budget · (1-r))
        其中 r = C_edge / (C_edge + C_cloud)
        """
        if C_edge + C_cloud < EPSILON:
            return f_edge_avail, f_cloud_avail

        r = C_edge / (C_edge + C_cloud)  # 边缘时间占比

        if r > EPSILON:
            f_edge = C_edge / (T_budget * r)
        else:
            f_edge = f_edge_avail

        if (1 - r) > EPSILON:
            f_cloud = C_cloud / (T_budget * (1 - r))
        else:
            f_cloud = f_cloud_avail

        return min(f_edge, f_edge_avail), min(f_cloud, f_cloud_avail)

    def _solve_case3(self, C_edge, C_cloud, E_budget, f_edge_avail, f_cloud_avail):
        """
        Case 3: 仅能量约束激活
        拉格朗日乘子法求解

        最优条件: 2κ_edge · f_edge · C_edge = 2κ_cloud · f_cloud · C_cloud
        即: f_cloud = f_edge · (κ_edge · C_edge) / (κ_cloud · C_cloud)
        """
        if C_edge < EPSILON and C_cloud < EPSILON:
            return f_edge_avail, f_cloud_avail

        # 纯边缘情况
        if C_cloud < EPSILON:
            f_edge = np.sqrt(E_budget / (self.kappa_edge * C_edge))
            return min(f_edge, f_edge_avail), f_cloud_avail

        # 纯云端情况
        if C_edge < EPSILON:
            f_cloud = np.sqrt(E_budget / (self.kappa_cloud * C_cloud))
            return f_edge_avail, min(f_cloud, f_cloud_avail)

        # 边缘-云端协同
        ratio = (self.kappa_edge * C_edge) / (self.kappa_cloud * C_cloud)
        A = self.kappa_edge * C_edge + self.kappa_cloud * C_cloud * (ratio ** 2)

        f_edge = np.sqrt(E_budget / A)
        f_cloud = f_edge * ratio

        return min(f_edge, f_edge_avail), min(f_cloud, f_cloud_avail)

    def _solve_case4(self, C_edge, C_cloud, T_budget, E_budget, f_edge_avail, f_cloud_avail):
        """
        Case 4: 两约束均激活
        联立时延等式和能量等式求解

        方程组:
            (1) C_edge/f_edge + C_cloud/f_cloud = T_budget
            (2) κ_edge · f_edge² · C_edge + κ_cloud · f_cloud² · C_cloud = E_budget

        求解策略: 二分法
        """
        def energy_residual(f_edge):
            """计算给定f_edge时的能量残差"""
            if f_edge < EPSILON:
                return float('inf')
            # 从时延约束求f_cloud
            T_edge = C_edge / f_edge
            T_cloud_budget = T_budget - T_edge
            if T_cloud_budget <= EPSILON:
                return float('inf')
            f_cloud = C_cloud / T_cloud_budget
            # 计算能量
            E = self.kappa_edge * (f_edge ** 2) * C_edge + \
                self.kappa_cloud * (f_cloud ** 2) * C_cloud
            return E - E_budget

        # 二分搜索
        f_edge_min = C_edge / T_budget
        f_edge_max = f_edge_avail

        for _ in range(50):
            f_mid = (f_edge_min + f_edge_max) / 2
            E_mid = energy_residual(f_mid)

            if abs(E_mid) < 1e-10:
                f_edge = f_mid
                break

            if E_mid > 0:
                f_edge_min = f_mid
            else:
                f_edge_max = f_mid

        # 计算对应的f_cloud
        T_edge = C_edge / f_edge
        T_cloud_budget = T_budget - T_edge
        f_cloud = C_cloud / T_cloud_budget

        return min(f_edge, f_edge_avail), min(f_cloud, f_cloud_avail)

    def optimize_allocation(self, C_edge, C_cloud, T_upload, T_trans, T_return, T_max):
        """
        优化资源分配

        算法流程:
            Step 1: 计算四组候选解
            Step 2: 可行性检查
            Step 3: 选择可行且时延最小者
        """
        # 计算四组候选解
        f1_edge, f1_cloud = self._solve_case1(f_edge_avail, f_cloud_avail)
        f2_edge, f2_cloud = self._solve_case2(C_edge, C_cloud, T_budget, ...)
        f3_edge, f3_cloud = self._solve_case3(C_edge, C_cloud, E_budget, ...)
        f4_edge, f4_cloud = self._solve_case4(C_edge, C_cloud, T_budget, E_budget, ...)

        # 可行性检查并选择最优
        candidates = [(f1_edge, f1_cloud, 1), (f2_edge, f2_cloud, 2),
                     (f3_edge, f3_cloud, 3), (f4_edge, f4_cloud, 4)]

        best_result = None
        min_delay = float('inf')

        for f_e, f_c, case_num in candidates:
            if self._check_feasibility(f_e, f_c, C_edge, C_cloud, T_budget, E_budget, ...):
                T_compute = C_edge / f_e + C_cloud / f_c
                if T_compute < min_delay:
                    min_delay = T_compute
                    best_result = (f_e, f_c, case_num)

        return AllocationResult(...)
```

### 3.3 四组闭式解汇总

| Case | 条件 | 公式 |
|------|------|------|
| Case 1 | 两约束均不激活 | `f_edge* = f_avail, f_cloud* = f_cloud_max` |
| Case 2 | 仅时延约束激活 | `f_edge* = C_edge / (T_budget · r)` |
| Case 3 | 仅能量约束激活 | `f_edge = sqrt(E_budget / A)` (拉格朗日乘子法) |
| Case 4 | 两约束均激活 | 二分法联立方程组求解 |

---

## 四、模块文件位置

| 模块 | 文件位置 |
|------|----------|
| 竞争比计算 | `algorithms/offline_optimal.py` |
| Active Inference - 状态空间 | `algorithms/active_inference/state_space.py` |
| Active Inference - 生成模型 | `algorithms/active_inference/generative_model.py` |
| Active Inference - 信念更新 | `algorithms/active_inference/belief_updater.py` |
| Active Inference - 轨迹预测 | `algorithms/active_inference/trajectory_predictor.py` |
| Active Inference - 自由能计算 | `algorithms/active_inference/free_energy.py` |
| Active Inference - 行动选择 | `algorithms/active_inference/action_selector.py` |
| Active Inference - 感知行动循环 | `algorithms/active_inference/perception_action_loop.py` |
| 闭式解求解 | `algorithms/phase2/resource_optimizer.py` |

---

## 五、参考文档

| 文档 | 用途 |
|------|------|
| `docs/竞争比.txt` | 竞争比计算说明 |
| `docs/自由能.txt` | Active Inference 理论 |
| `docs/idea118.txt` | 闭式解计算 (2.6节) |

---

*文档生成时间: 2026-02-24*
